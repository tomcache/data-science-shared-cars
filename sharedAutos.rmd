---
title: "Shared Car Service Availability Study"
author: "Thomas Linnell"
date: "9/10/2019"
output: html_document
---

## Introduction
As part of a program to improve the livability of its environs, the city of Tel Aviv, Israel has developed a car sharing service (think Zipcar) called AutoTel that allows users to take a car from an available location whenever they wish and use it for transportation. When finished, they can park it at the nearest designated spot near their destination that is open. Since this service depends on the users to distribute cars around the city, and since demand patterns frequently shift, it is important to study the usage patterns in order to be able to provide enough cars in the right places to insure that most users can utilize this service when they need it.

This project will use data from a database available on kaggle that contains data on the avaialability of cars in Tel Aviv from xxx to yyy. We will examine this data and modify it to make it more useful for our analysis, and will provide some observations on usage patterns.

What this data does not provide, however is any information on the users demand, other than by proxy when a car is moved from a particular location (if someone wants to use a car in a particular neighborhood, but there is no car nearby, we don't see that information). However, we will develop a model, using machine learning techniques, in order to predict the availability of cars.

### Project setup and environment

Check for required packages and install when necessary. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Here we check for and load all of the required packages to run the project.

if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(ggmap)) install.packages("ggmap", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

```

### Data

This project will utilize the dataset _shared_table_, a sample of a Big Query dataset available on Kaggle provided by DoIt International, and can be found  [here](https://www.kaggle.com/doit-intl/autotel-shared-car-locations).

For the purposes of this project, a copy of this dataset is also provided in the github project here, since Kaggle requires an authenticated account in order to access the data. 

This dataset sample contains one months worth of data on parked cars in the following format:

timestamp, geographic location, # of cars in location, list of car identifiers

We will load the data and adjust it into the format that we want to use to analyze the usage patterns:

```{r load-data, warning = FALSE}

dl <- tempfile()
download.file("https://www.kaggle.com/doit-intl/autotel-shared-car-locations/downloads/autotel-shared-car-locations.zip", dl)



db <- read_csv("autotel//sample_table.csv")

```


We first take a look at the structure, definition and layout of the sample data:
```{r view-data}

glimpse(db)
summary(db)

min(db$timestamp)
max(db$timestamp)

length(unique(db$carsList))

length(unique(db$latitude))

length(unique(db$longitude))

length(unique(db$total_cars))

```

Our first task will be to arrange the data to make it more useful for our analysis. We want to correct the timestamp format, and we will add columns to allow us to group observations by time, day of week, and we also adjust the timestamps for local time. Finally, we add columns to allow grouping by location.

```{r tidying-data}

db <- db %>% 
  mutate(timestamp = as.POSIXct(timestamp)) %>% 
  mutate(timestamp = timestamp + 3600*2, # local time = GMT/UTC + 2
         Hour = lubridate::hour(timestamp),
         Minute = lubridate::minute(timestamp),
         Weekday = lubridate::wday(timestamp)
  )

# We next add a grid identifier, using a grid size of approximately 100m square

db <- db %>%
  mutate(Grid_lat = round(latitude,3),
         Grid_long = round(longitude,3),
         gridKey = (Grid_lat*10000000000 + Grid_long*10000)
 )

# We also can use a calculation of the number of cars in use / available at any given time:

dbCarUse <- db %>%
  group_by(timestamp) %>% summarise(total_cars = sum(total_cars)) 

dbCarUse <- dbCarUse %>%
  mutate(timestamp = timestamp + 3600*2, # local time = GMT/UTC + 2
         Hour = lubridate::hour(timestamp),
         Minute = lubridate::minute(timestamp),
         label_wday = lubridate::wday(timestamp, label = TRUE, abbr = FALSE)
  )

glimpse(db)

```

### Analysis

We now want to examine the data in more detail to see what it can tell us about the usage patterns for the shared car service. Since we have the number and location of all of the shared cars that are not in use for each timestamp entry, we can use this to plot the availability of cars across the fleet for the sample time period:


```{r analysis-availability}

db %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars)) %>%
    ggplot(aes(timestamp, cars_available)) + geom_line(color = "slategray4", size = 1.0) +
    geom_smooth(method="auto", se=TRUE, fullrange=FALSE, level=0.95) +
  ggtitle("Shared Car Availability in Tel Aviv") 

all_cars <- db %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars))

(fleet <-  max(all_cars$cars_available))

```

From this plot, we can see that the demand chages periodically throughout the day, and from the smoothed plot we can also see that the demand is periodic with the day of the week. We also see a dropout around December 18, which as it turns out is a national holiday in Israel. It is also likely that there are effects for other holiday observations during this timeframe which would be more apparent if we had a wider range of samples, but these will not be periodic in nature. 

We'll next take a look in more detail at the apparent periodicity to confirm our intuition:

```{r analysis-availability-detail}

# We can plot the utilization of the fleet over time:
dbCarUse <- dbCarUse %>% mutate(utilize = 1 - total_cars/fleet)

dbCarUse %>% ggplot(aes(timestamp, utilize)) + geom_line(color = "slategray4", size = 1.0)


dbCarUse %>% group_by(Hour) %>% summarize(cars_available = sum(total_cars)) %>%
    ggplot(aes(Hour, cars_available)) + 
  geom_bar(stat = "identity", color = "steelblue2", fill = "lightsteelblue3") +
  ggtitle("Shared Car Availability in Tel Aviv by Hour") 


dbCarUse %>% group_by(label_wday) %>% 
    ggplot(aes(label_wday, total_cars, fill = label_wday)) + 
  geom_boxplot(aes(group = label_wday)) +
  ggtitle("Shared Car Availability in Tel Aviv by Weekday") 

(Utilize_min <- min(dbCarUse$utilize))

(Utilize_max <- max(dbCarUse$utilize))

```


By looking at a single 24-hour period for a weekday, we can also observe that the demand for cars starts to occur around 6am, has several local peaks during the day, relaxes around lunch and dinnertime, and surprisingly the peak demand for the day occurs around 9pm before tapering off and going through the cycle again.

```{r analysis-availability-day}

sample_day1 <- ymd("2018-12-15")

db_day1 <- db %>% filter(as_date(timestamp) == sample_day1 )


db_day1 %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars)) %>%
  ggplot(aes(timestamp, cars_available)) + geom_line(color = "slategray4", size = 1.0) +
  ggtitle("Car Availability for Tuesday, December 15, 2018")
  

```

We can also plot some car locations to see how they are distributed across the city, and how this varies by hour, to get a sense of the demand for the service. In order to accomplish this, we will need to get an understanding of the identity of the cars in each location. From our summary of the database, above, we know that there are a maximum of 10 cars in each location per timestamp, so we use that to construct our list:

```{r analysis-location-day}

dbcars <-  db %>% 
  select(carsList) %>% 
  distinct() %>% 
  mutate(cars = sub(carsList, pattern = "\\[", replacement = "")) %>% 
  mutate(cars = sub(cars, pattern = "\\]", replacement = "")) %>% 
  separate(cars, sep = ",", into = c("car1", "car2", "car3", "car4", "car5", "car6", "car7", "car8", "car9", "car10"), fill = "right") %>% 
  mutate_at(2:11, as.numeric)
# And, let's have a list of all cars "names" (which is a number)
CarsList <- (1:max(dbcars$car1, na.rm = T))[1:max(dbcars$car1, na.rm = T) %in% dbcars$car1]
# While there are 261 cars, some numbers are not in use (probably anymore), and we'll find to have the last car as # 272

# Now we can create a dataframe where each row is only for a specifc car, whose number will be in a new column: "Car"
# Also, we don't need to have the data on each car every 2 minutes, as most of the day it does not move
# So we'll have a row only for each change in location of a car

# create a list of entries separated by car, and add grid information:

# this grid is coarse, roughly 1km^2 in area:

dbByCar <- left_join(db, dbcars) %>% 
  gather(CarI, Car, car1:car10)  %>% 
  filter(!is.na(Car)) %>% 
  group_by(Car) %>% 
  arrange(timestamp) %>%
  ungroup()

#n We'll limit our data to a single day to keep it from becoming too unweildy. We will also calculate a summary of the number of cars visited in each grid per hour:


flowByGrid <- dbByCar %>% 
  filter(as_date(timestamp) == sample_day1 ) %>%
  group_by(Grid_lat, Grid_long, Hour) %>%
  summarize(inventory = n_distinct(Car)
  )

flowByGrid %>% ggplot(aes(Grid_long, Grid_lat, color = inventory)) + 
  geom_point(size = 3) +
  scale_color_gradient2(midpoint = 1, low = "blue",  mid = "gold",
                        high = "red", space = "Lab" ) +
  facet_wrap(~Hour)



```


And through the magic of ggmaps, we can plot these locations over a map of the city to see 

```{r analysis-location-map}

# First, define the box for Tel Aviv:
# tav_top <- 32.1500
# tav_left <- 34.7000
# tav_bot <- 32.0000
# tav_right <- 34.8500

tav_top <- 32.1500
tav_left <- 34.7200
tav_bot <- 32.0000
tav_right <- 34.8500

tav_borders <- c(left = tav_left,
                 right = tav_right,
                 top = tav_top,
                 bottom = tav_bot)

tav_map <- get_stamenmap(bbox = tav_borders, zoom = 11)

p <- ggmap(tav_map)

# Calculate the change in inventory by grid per hour:

flowByGrid <- 
  flowByGrid %>%
  group_by(Grid_lat, Grid_long) %>%
  mutate(carFlow = inventory - dplyr::lag(inventory, n = 1, default = NA, order_by = Hour))


m1 <- flowByGrid %>% filter(carFlow != 0 & Hour == 12)

m2 <- flowByGrid %>% filter(carFlow != 0 & Hour == 20)

p + geom_point(aes(Grid_long, Grid_lat, color = carFlow), data = m1, size = 3) +
  scale_color_gradient2(midpoint = 0, low = "blue", mid = "slategray4",
                        high = "red", space = "Lab" ) +
  labs(title = "Tel Aviv car locations 12/15/18 12pm")

p + geom_point(aes(Grid_long, Grid_lat, color = carFlow), data = m2, size = 3) +
  scale_color_gradient2(midpoint = 0, low = "blue", mid = "slategray4",
                        high = "red", space = "Lab" ) +
  labs(title = "Tel Aviv car locations 12/15/18 8pm")



```


#### Availability patterns

Let's look at the distribution of the total number of cars by location. We can see from this chart that the distribution is not random, with some locations having many more cars fill spots than others. 

```{r analysis-location-dist}
db_cars <- db_day1 %>% group_by(gridKey) %>% summarize(cars_available = sum(total_cars))

db_cars$gridKey <- factor(db_cars$gridKey, levels = db_cars$gridKey[order(-db_cars$cars_available)])

ggplot(data = db_cars, aes(x = gridKey, y = cars_available)) + 
  geom_bar(stat = "identity", color = "slategray4")

```


### Availability Prediction

Now that we have gotten some idea of the usage patterns in the data, we want to build a prediction system for cars in specific locations.

We start by building a series of training and test sets.


```{r prediction-testsets}

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db$total_cars, times = 1, p = 0.1, list = FALSE)
db_train <- db[-test_index,]
db_test <- db[test_index,]

db_test <- db_test %>% 
  semi_join(db_train, by = "gridKey") 

# We will now create a single-day data set, and do the same thing:

sample_day1 <- ymd("2018-12-15")

db_day <- db %>% filter(as_date(timestamp) == sample_day1 )

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db_day$total_cars, times = 1, p = 0.1, list = FALSE)
db_day_train <- db_day[-test_index,]
db_day_test <- db_day[test_index,]

# and a 1-week data set, from 12/20/18 through 12/26/18:
week_day1 <- ymd("2018-12-20")
week_day7 <- ymd("2018-12-26")

db_week <- db %>% filter(timestamp >= week_day1 & timestamp <= week_day7 )

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db_week$total_cars, times = 1, p = 0.1, list = FALSE)
db_week_train <- db_week[-test_index,]
db_week_test <- db_week[test_index,]


```


Now that we have our test sets, we will start building up a prediction model. 

```{r prediction-initial}


RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}


dbTrainMean <- db_train %>% summarize(Mean = mean(total_cars)) 
mu_dbTrain <- mean(db_train$total_cars)

db_dayTrainMean <- db_day_train %>% summarize(Mean = mean(total_cars)) 
mu_db_dayTrain <- mean(db_day_train$total_cars)

db_weekTrainMean <- db_week_train %>% summarize(Mean = mean(total_cars)) 
mu_db_weekTrain <- mean(db_week_train$total_cars)

(baseline_rmse <- RMSE(db_test$total_cars, mu_dbTrain))

(baseline_day_rmse <- RMSE(db_day_test$total_cars, mu_db_dayTrain))

(baseline_week_rmse <- RMSE(db_week_test$total_cars, mu_db_weekTrain))


```


We now will calculate a naive predictor, utilizing a linear regression model. Since we have detected variability associated with location, time of day, and day of week, we will select these as factors for our calculation:

```{r prediction-linear}

fit_dayH <- lm(total_cars ~ as.factor(Hour), data = db_day_train)

fit_day

summary(fit_day)

fit_dayL <- lm(total_cars ~ as.factor(gridKey), data = db_day_train)

fit_dayL




pred_day <- lm(total_cars ~ Hour + gridKey + Weekday, data = db_day_train)

summary(pred_day)

predictions_day <- predict(pred_day, db_day_test)

RMSE(db_day_test$total_cars, predictions_day)


pred_week <- lm(total_cars ~ Hour + gridKey + Weekday, data = db_week_train)

predictions_week <- predict(pred_week, db_week_test)

RMSE(db_week_test$total_cars, predictions_week)


pred_full <- lm(total_cars ~ Hour + gridKey + Weekday, data = db_train)

predictions_full <- predict(pred_full, db_test)

RMSE(db_test$total_cars, predictions_full)


```


Now we will move on to knn modeling:

```{r prediction-knn}

train_knn1 <- train(total_cars ~ Hour + gridKey + Weekday, method = "knn", data = dbg1_train)

y_hat_knn <- predict(train_knn1, dbg1_test, type = "raw")


train_knnW <- train(total_cars ~ Hour + gridKey + Weekday, method = "knn", data = db_week_train)

y_hat_knnW <- predict(train_knnW, db_week_test, type = "raw")


```

```{r next}


```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.




