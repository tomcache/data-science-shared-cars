---
title: "Shared Car Service Availability Study"
author: "Thomas Linnell"
date: "9/10/2019"
output: html_document
---

## Introduction
As part of a program to improve the livability of its environs, the city of Tel Aviv, Israel has developed a car sharing service (think Zipcar) called AutoTel that allows users to take a car from an available location whenever they wish and use it for transportation. When finished, they can park it at the nearest designated spot near their destination that is open. Since this service depends on the users to distribute cars around the city, and since demand patterns frequently shift, it is important to study the usage patterns in order to be able to provide enough cars in the right places to insure that most users can utilize this service when they need it.

This project will use data from a database available on kaggle that contains data on the avaialability of cars in Tel Aviv from mid-December, 2018 to mid-January, 2019. We will examine this data and modify it to make it more useful for our analysis, and will provide some observations on usage patterns.

What this data does not provide, however is any information on the users demand, other than by proxy when a car is moved from a particular location (if someone wants to use a car in a particular neighborhood, but there is no car nearby, we don't see that information). However, we will develop a model, using machine learning techniques, in order to predict the availability of cars.

### Project setup and environment

Check for required packages and install them when necessary. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Here we check for and load all of the required packages to run the project.

if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(ggmap)) install.packages("ggmap", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

```

### Data

This project will utilize the dataset _shared_table_, a sample of a Big Query dataset available on Kaggle provided by Do-It International, and can be found  [here](https://www.kaggle.com/doit-intl/autotel-shared-car-locations).

For the purposes of this project, a copy of this dataset is also provided in the github project since Kaggle requires an authenticated account in order to access the data. 

This dataset sample contains one months worth of data on parked cars in the following format:

timestamp, geographic location, # of cars in location, list of car identifiers

We will load the data and adjust it into the format that we want to use to analyze the usage patterns.

Download the sample data, unzip the file and convert it for use in R:

```{r load-data, warning = FALSE}


dl <- tempfile()

dURL <- "https://github.com/tomcache/data-science-shared-cars/raw/master/datasource/autotel-shared-car-locations.zip"

download.file(dURL, destfile = dl, method = "wininet")

unzip(dl, overwrite = TRUE, exdir = "autotel")

db <- read_csv("autotel//sample_table.csv")

```


We first take a look at the structure, definition and layout of the sample data. We see from the below that we have a total of about 6.7 million observations of parked car locations and times. From the summary, we also see that the number of cars in each location can vary from 0 to 10 cars. 

We also pull information from the timestamps to get the start and end date of the observations. Since the data is from December 11 through January 10, we expect that there may be effects due to holiday and end of year celebrations.


```{r view-data}

glimpse(db)
summary(db)

min(db$timestamp)
max(db$timestamp)


```

Our first task will be to arrange the data to make it more useful for our analysis. We want to correct the timestamp format, and we will add columns to allow us to group observations by time and day of the week. We will also adjust the timestamps for local time. Finally, we add columns to allow grouping by location.

In order to facilitate the location analysis, we create a grid structure of approximately
110 x 90m square. We also create a unique key for each grid to allow us to easily index by this metric. 

```{r tidying-data}

db <- db %>% 
  mutate(timestamp = as.POSIXct(timestamp)) %>% 
  mutate(timestamp = timestamp + 3600*2, # local time = GMT/UTC + 2
         Hour = lubridate::hour(timestamp),
         Minute = lubridate::minute(timestamp),
         Weekday = lubridate::wday(timestamp)
  )

# We next add a grid identifier, using a grid size of approximately 100m square

db <- db %>%
  mutate(Grid_lat = round(latitude,3),
         Grid_long = round(longitude,3),
         gridKey = (Grid_lat*10000000000 + Grid_long*10000)
 )

# We also can use a calculation of the number of cars in use / available at any given time:

dbCarUse <- db %>%
  group_by(timestamp) %>% summarise(total_cars = sum(total_cars)) 

dbCarUse <- dbCarUse %>%
  mutate(timestamp = timestamp + 3600*2, # local time = GMT/UTC + 2
         Hour = lubridate::hour(timestamp),
         Minute = lubridate::minute(timestamp),
         label_wday = lubridate::wday(timestamp, label = TRUE, abbr = FALSE)
  )

glimpse(db)

```

### Analysis

We now want to examine the data in more detail to see what it can tell us about the usage patterns for the shared car service. Since we have the number and location of all of the shared cars that are not in use for each timestamp entry, we can use this to plot the availability of cars across the fleet for the sample time period.

We see several patterns in this data. From this plot, we can see that there is a daily cycle that the demand follows. We also see a dropout around December 18, which as it turns out is a national holiday in Israel. 

By looking at the smoothed plot, we can also discern that there is a weekly cycle to car demand as well. 

It is also likely that there are effects for other holiday observations during this timeframe which would be more apparent if we had a wider range of samples, but these will not be periodic in nature, as is the case for the previously noted national holiday. 


```{r analysis-availability, fig.align = 'center', fig.width = 6, fig.height = 4}

db %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars)) %>%
    ggplot(aes(timestamp, cars_available)) + geom_line(color = "slategray4", size = 1.0) +
    geom_smooth(method="auto", se=TRUE, fullrange=FALSE, level=0.95) +
  ggtitle("Shared Car Availability in Tel Aviv") 

all_cars <- db %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars))

(fleet <-  max(all_cars$cars_available))

```

Let's dive a little further into the details of this data.

By looking at a single 24-hour period for a weekday, we can observe that the demand for cars starts to occur around 6am, has several local peaks during the day, relaxes around lunch and dinnertime, and surprisingly the peak demand for the day occurs around 9pm before tapering off and going through the cycle again.

```{r analysis-availability-day}

sample_day1 <- ymd("2018-12-15")

db_day1 <- db %>% filter(as_date(timestamp) == sample_day1 )


db_day1 %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars)) %>%
  ggplot(aes(timestamp, cars_available)) + geom_line(color = "slategray4", size = 1.0) +
  ggtitle("Car Availability for Tuesday, December 15, 2018")
  

```

We'll next take a look in more detail at the apparent periodicity to confirm our intuition:

```{r analysis-availability-detail}

# We can plot the utilization of the fleet over time:
dbCarUse <- dbCarUse %>% mutate(utilize = 1 - total_cars/fleet)

dbCarUse %>% ggplot(aes(x = utilize, fill = label_wday)) + 
  geom_density(position = "identity", alpha = 0.6) +
  ggtitle("Density Plot of Shared Car Fleet Utilization") 

dbCarUse %>% group_by(Hour) %>% summarize(cars_available = sum(total_cars)) %>%
    ggplot(aes(Hour, cars_available)) + 
  geom_bar(stat = "identity", color = "steelblue2", fill = "lightsteelblue3") +
  ggtitle("Shared Car Availability in Tel Aviv by Hour") 


dbCarUse %>% group_by(label_wday) %>% 
    ggplot(aes(label_wday, total_cars, fill = label_wday)) + 
  geom_boxplot(aes(group = label_wday)) +
  ggtitle("Shared Car Availability in Tel Aviv by Weekday") 

(Utilize_min <- min(dbCarUse$utilize))

(Utilize_max <- max(dbCarUse$utilize))

```




We can also plot some car locations to see how they are distributed across the city, and how this varies by hour, to get a sense of the demand for the service. In order to accomplish this, we will need to get an understanding of the identity of the cars in each location. From our summary of the database, above, we know that there are a maximum of 10 cars in each location per timestamp, so we use that to construct our list:

```{r analysis-location-day}

dbcars <-  db %>% 
  select(carsList) %>% 
  distinct() %>% 
  mutate(cars = sub(carsList, pattern = "\\[", replacement = "")) %>% 
  mutate(cars = sub(cars, pattern = "\\]", replacement = "")) %>% 
  separate(cars, sep = ",", into = c("car1", "car2", "car3", "car4", "car5", "car6", "car7", "car8", "car9", "car10"), fill = "right") %>% 
  mutate_at(2:11, as.numeric)
# And, let's have a list of all cars "names" (which is a number)
CarsList <- (1:max(dbcars$car1, na.rm = T))[1:max(dbcars$car1, na.rm = T) %in% dbcars$car1]
# While there are 261 cars, some numbers are not in use (probably anymore), and we'll find to have the last car as # 272

# Now we can create a dataframe where each row is only for a specifc car, whose number will be in a new column: "Car"
# Also, we don't need to have the data on each car every 2 minutes, as most of the day it does not move
# So we'll have a row only for each change in location of a car

# create a list of entries separated by car, and add grid information:

# this grid is coarse, roughly 1km^2 in area:

dbByCar <- left_join(db, dbcars) %>% 
  gather(CarI, Car, car1:car10)  %>% 
  filter(!is.na(Car)) %>% 
  group_by(Car) %>% 
  arrange(timestamp) %>%
  ungroup()

#n We'll limit our data to a single day to keep it from becoming too unweildy. We will also calculate a summary of the number of cars visited in each grid per hour:


flowByGrid <- dbByCar %>% 
  filter(as_date(timestamp) == sample_day1 ) %>%
  group_by(Grid_lat, Grid_long, Hour) %>%
  summarize(inventory = n_distinct(Car)
  )

flowByGrid %>% ggplot(aes(Grid_long, Grid_lat, color = inventory, size = inventory)) + 
  geom_point() +
  scale_color_gradient2(midpoint = 2.5, low = "blue",  mid = "gold",
                        high = "red", space = "Lab" )
  # facet_wrap(~Hour)


```


And through the magic of ggmaps, we can plot these locations over a map of the city to see 

```{r analysis-location-map}

# First, define the box for Tel Aviv:
# tav_top <- 32.1500
# tav_left <- 34.7000
# tav_bot <- 32.0000
# tav_right <- 34.8500

tav_top <- 32.1500
tav_left <- 34.7350
tav_bot <- 32.0300
tav_right <- 34.8500

tav_borders <- c(left = tav_left,
                 right = tav_right,
                 top = tav_top,
                 bottom = tav_bot)

tav_map <- get_stamenmap(bbox = tav_borders, zoom = 11)

p <- ggmap(tav_map)

p + 
  geom_point(aes(Grid_long, Grid_lat, color = inventory, size = inventory), data = flowByGrid) +
  scale_color_gradient2(midpoint = 2.5, low = "blue",  mid = "gold",
                        high = "red", space = "Lab" )

```




#### Availability patterns

Let's look at the distribution of the total number of cars by location. We can see from this chart that the distribution is not random, with some locations having many more cars fill spots than others. 

```{r analysis-location-dist}
db_cars <- db_day1 %>% group_by(gridKey) %>% summarize(cars_available = sum(total_cars))

db_cars$gridKey <- factor(db_cars$gridKey, levels = db_cars$gridKey[order(-db_cars$cars_available)])

ggplot(data = db_cars, aes(x = gridKey, y = cars_available)) + 
  geom_bar(stat = "identity", color = "slategray4")

```


### Availability Prediction

Now that we have gotten some idea of the usage patterns in the data, we want to build a prediction system for cars in specific locations.

We start by building a series of training and test sets.


```{r prediction-testsets}

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db$total_cars, times = 1, p = 0.1, list = FALSE)
db_train <- db[-test_index,]
db_test <- db[test_index,]

db_test <- db_test %>% 
  semi_join(db_train, by = "gridKey") %>%
  semi_join(db_train, by = "Hour") %>%
  semi_join(db_train, by = "Weekday")

# We will now create a single-day data set, and do the same thing:

sample_day1 <- ymd("2018-12-15")

db_day <- db %>% filter(as_date(timestamp) == sample_day1 )

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db_day$total_cars, times = 1, p = 0.1, list = FALSE)
db_day_train <- db_day[-test_index,]
db_day_test <- db_day[test_index,]

db_day_test <- db_day_test %>% 
  semi_join(db_day_train, by = "gridKey") %>%
  semi_join(db_day_train, by = "Hour") %>%
  semi_join(db_day_train, by = "Weekday")

# and a 1-week data set, from 12/20/18 through 12/26/18:
week_day1 <- ymd("2018-12-20")
week_day7 <- ymd("2018-12-26")

db_week <- db %>% filter(timestamp >= week_day1 & timestamp <= week_day7 )

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db_week$total_cars, times = 1, p = 0.1, list = FALSE)
db_week_train <- db_week[-test_index,]
db_week_test <- db_week[test_index,]

db_week_test <- db_week_test %>% 
  semi_join(db_week_train, by = "gridKey") %>%
  semi_join(db_week_train, by = "Hour") %>%
  semi_join(db_week_train, by = "Weekday")

```


Now that we have our test sets, we will start building up a prediction model. 

```{r prediction-initial}

# function for calculating RMSE of prediction vs. actual

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

# Caclculation of simple means for each time epoch:

dbTrainMean <- db_train %>% summarize(Mean = mean(total_cars)) 
mu_dbTrain <- mean(db_train$total_cars)

db_dayTrainMean <- db_day_train %>% summarize(Mean = mean(total_cars)) 
mu_db_dayTrain <- mean(db_day_train$total_cars)

db_weekTrainMean <- db_week_train %>% summarize(Mean = mean(total_cars)) 
mu_db_weekTrain <- mean(db_week_train$total_cars)

(baseline_rmse <- RMSE(db_test$total_cars, mu_dbTrain))

(baseline_day_rmse <- RMSE(db_day_test$total_cars, mu_db_dayTrain))

(baseline_week_rmse <- RMSE(db_week_test$total_cars, mu_db_weekTrain))


```


We now will calculate a naive predictor, utilizing a linear regression model. Since we have detected variability associated with location, time of day, and day of week, we will select these as factors for our calculation:

```{r prediction-linear-day}

# build up a model for the day data set

# First, the hour effect:


day_hour_avgs <- db_day_train %>% 
  group_by(Hour) %>% 
  summarize(b_i_day = mean(total_cars - mu_db_dayTrain))

# day_hour_avgs %>% qplot(b_i, geom ="histogram", data = ., color = I("slategray4"))


predicted_ratings_day <- mu_db_dayTrain + db_day_test %>% 
  left_join(day_hour_avgs, by='Hour') %>%
  pull(b_i_day)

model_1_day_rmse <- RMSE(predicted_ratings_day, db_day_test$total_cars)

# Then we add the location effect:

day_grid_avgs <- db_day_train %>% 
  left_join(day_hour_avgs, by = 'Hour') %>%
  group_by(gridKey) %>% 
  summarize(b_u_day = mean(total_cars - mu_db_dayTrain - b_i_day))

predicted_ratings_day <- db_day_test %>% 
  left_join(day_hour_avgs, by='Hour') %>%
  left_join(day_grid_avgs, by='gridKey') %>%
  mutate(pred = mu_db_dayTrain + b_i_day + b_u_day) %>%
  pull(pred)


model_2_day_rmse <- RMSE(predicted_ratings_day, db_day_test$total_cars)

```

```{r prediction-linear-week}

# Now we do the same modeling for the week data set, and the full data set:

# First, the hour effect:


week_hour_avgs <- db_week_train %>% 
  group_by(Hour) %>% 
  summarize(b_i_week = mean(total_cars - mu_db_weekTrain))

# week_hour_avgs %>% qplot(b_i_week, geom ="histogram", data = ., color = I("slategray4"))


predicted_ratings_week <- mu_db_weekTrain + db_week_test %>% 
  left_join(week_hour_avgs, by='Hour') %>%
  pull(b_i_week)

model_1_week_rmse <- RMSE(predicted_ratings_week, db_week_test$total_cars)

# Then we add the location effect:

week_grid_avgs <- db_week_train %>% 
  left_join(week_hour_avgs, by = 'Hour') %>%
  group_by(gridKey) %>% 
  summarize(b_u_week = mean(total_cars - mu_db_weekTrain - b_i_week))

predicted_ratings_week <- db_week_test %>% 
  left_join(week_hour_avgs, by='Hour') %>%
  left_join(week_grid_avgs, by='gridKey') %>%
  mutate(pred = mu_db_dayTrain + b_i_week + b_u_week) %>%
  pull(pred)


model_2_week_rmse <- RMSE(predicted_ratings_week, db_week_test$total_cars)

# Then the weekday effect:

week_day_avgs <- db_week_train %>% 
  left_join(week_hour_avgs, by = 'Hour') %>%
  left_join(week_grid_avgs, by='gridKey') %>%
  group_by(Weekday) %>% 
  summarize(b_d_week = mean(total_cars - mu_db_weekTrain - b_i_week - b_u_week))

predicted_ratings_week <- db_week_test %>% 
  left_join(week_hour_avgs, by='Hour') %>%
  left_join(week_grid_avgs, by='gridKey') %>%
  left_join(week_day_avgs, by = 'Weekday') %>%
  mutate(pred = mu_db_dayTrain + b_i_week + b_u_week + b_d_week) %>%
  pull(pred)

model_3_week_rmse <- RMSE(predicted_ratings_week, db_week_test$total_cars)

```

```{r prediction-linear-full}


# First, the hour effect:


hour_avgs <- db_train %>% 
  group_by(Hour) %>% 
  summarize(b_i = mean(total_cars - mu_dbTrain))

# hour_avgs %>% qplot(b_i, geom ="histogram", data = ., color = I("slategray4"))


predicted_ratings <- mu_dbTrain + db_test %>% 
  left_join(hour_avgs, by='Hour') %>%
  pull(b_i)

model_1_rmse <- RMSE(predicted_ratings, db_test$total_cars)

# Then we add the location effect:

grid_avgs <- db_train %>% 
  left_join(hour_avgs, by = 'Hour') %>%
  group_by(gridKey) %>% 
  summarize(b_u = mean(total_cars - mu_dbTrain - b_i))

predicted_ratings <- db_test %>% 
  left_join(hour_avgs, by='Hour') %>%
  left_join(grid_avgs, by='gridKey') %>%
  mutate(pred = mu_dbTrain + b_i + b_u) %>%
  pull(pred)


model_2_rmse <- RMSE(predicted_ratings, db_test$total_cars)

# Then the weekday effect:

day_avgs <- db_train %>% 
  left_join(hour_avgs, by = 'Hour') %>%
  left_join(grid_avgs, by='gridKey') %>%
  group_by(Weekday) %>% 
  summarize(b_d = mean(total_cars - mu_dbTrain - b_i - b_u))

predicted_ratings <- db_test %>% 
  left_join(hour_avgs, by='Hour') %>%
  left_join(grid_avgs, by='gridKey') %>%
  left_join(day_avgs, by = 'Weekday') %>%
  mutate(pred = mu_dbTrain + b_i + b_u + b_d) %>%
  pull(pred)

model_3_rmse <- RMSE(predicted_ratings, db_test$total_cars)


```






Now we will move on to knn modeling:

```{r prediction-knn}

# Model: day

train_knnD <- train(total_cars ~ Hour + gridKey, method = "knn", data = db_day_train)

y_hat_knnD <- predict(train_knnD, db_day_test, type = "raw")

plot(train_knnD)

train_knnD$bestTune

model_knn_day <- RMSE(y_hat_knnD, db_day_test$total_cars)


```

```{r results-review}


```


```{r next}


```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.




