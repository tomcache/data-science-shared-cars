---
title: "Shared Car Service Availability Study"
author: "Thomas E Linnell"
date: "9/28/2019"
output: html_document
---

## Introduction
As part of a program to improve the livability of its environs, the city of Tel Aviv, Israel has developed a car sharing service (think Zipcar combined with Blue Bikes) called AutoTel that allows users to take a car from an available location whenever they wish and use it for transportation. When finished, they can park it at the nearest designated spot near their destination that is open. Since this service depends on the users to distribute cars around the city, and since demand patterns frequently shift, it is important to study the usage patterns in order to be able to provide enough cars in the right places to insure that most users can utilize this service when they need it.

This project will use data from a database available on kaggle that contains data on the avaialability of cars in Tel Aviv from mid-December, 2018 to mid-January, 2019. We will examine this data and modify it to make it more useful for our analysis, and will provide some observations on usage patterns.

What this data does not provide, however is any information on the users demand, other than by proxy when a car is moved from a particular location (if someone wants to use a car in a particular neighborhood, but there is no car nearby, we don't see that information). However, we will develop a model, using machine learning techniques, in order to predict the availability of cars.

We will also investigate the performance of an alternative model to provide comparisons to our initial attempt and to see if improvements in accuracy are possible.

### Project setup and environment

Check for required packages and install them when necessary. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Here we check for and load all of the required packages to run the project.

if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(ggmap)) install.packages("ggmap", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

```

### Data

This project will utilize the dataset _shared_table_, in csv format, which is a sample of a Big Query dataset available on Kaggle provided by Do-It International, and can be found  [here](https://www.kaggle.com/doit-intl/autotel-shared-car-locations).

For the purposes of this project, a copy of this dataset is also provided in the github project repository since Kaggle requires an authenticated account in order to access the data. 

We will load the data and adjust it into the format that we want to use to analyze the usage patterns.

Download the sample data, unzip the file and convert it for use in R:

```{r load-data, warning = FALSE}


dl <- tempfile()

dURL <- "https://github.com/tomcache/data-science-shared-cars/raw/master/datasource/autotel-shared-car-locations.zip"

download.file(dURL, destfile = dl, method = "wininet")

unzip(dl, overwrite = TRUE, exdir = "autotel")

# This dataset sample contains one months worth of data on parked cars in the format shown below.


db <- read_csv("autotel//sample_table.csv")

```

#### Review

We first take a look at the structure, definition and layout of the sample data. We see from the below that we have a total of about 6.7 million observations of parked car locations and times. From the summary, we also see that the number of cars in each location can vary from 0 to 10. 

```{r view-data}

glimpse(db)
summary(db)

```

We also pull information from the timestamps to get the start and end date of the observations. Since the data is from December 11 through January 10, we expect that there may be effects due to holiday and end of year celebrations.

```{r view-data-timestamp}

min(db$timestamp)
max(db$timestamp)

```

#### Tidy the data


Our first task will be to arrange the data to make it more useful for our analysis. We want to correct the timestamp format, and we will add columns to allow us to group observations by time and day of the week. We will also adjust the timestamps for local time. Finally, we add columns to allow grouping by location.

In order to facilitate the location analysis, we create a grid structure of approximately
110 x 90m square. We also create a unique key for each grid to allow us to easily index by this metric. 

```{r tidying-data}

db <- db %>% 
  mutate(timestamp = as.POSIXct(timestamp)) %>% 
  mutate(timestamp = timestamp + 3600*2, # local time = GMT/UTC + 2
         Hour = lubridate::hour(timestamp),
         Minute = lubridate::minute(timestamp),
         Weekday = lubridate::wday(timestamp)
  )

# We next add a grid identifier, using a grid size of approximately 100m square

db <- db %>%
  mutate(Grid_lat = round(latitude,3),
         Grid_long = round(longitude,3),
         gridKey = (Grid_lat*10000000000 + Grid_long*10000)
 )

# We also can use a calculation of the number of cars in use / available at any given time:

dbCarUse <- db %>%
  group_by(timestamp) %>% summarise(total_cars = sum(total_cars)) 

dbCarUse <- dbCarUse %>%
  mutate(timestamp = timestamp + 3600*2, # local time = GMT/UTC + 2
         Hour = lubridate::hour(timestamp),
         Minute = lubridate::minute(timestamp),
         label_wday = lubridate::wday(timestamp, label = TRUE, abbr = FALSE)
  )


```

A quick review of the changes that we have made:


```{r tidying-data-result}

glimpse(db)

```

### Analysis

We now want to examine the data in more detail to see what it can tell us about the usage patterns for the shared car service. Since we have the number and location of all of the shared cars that are not in use for each timestamp entry, we can use this to plot the availability of cars across the fleet for the sample time period.

We see several patterns in this data. From this plot, we can see that there is a daily cycle that the demand follows. We also see a dropout around December 18, which as it turns out is a national holiday in Israel. 

By looking at the smoothed plot, we can also discern that there is a weekly cycle to car demand as well. 

It is also likely that there are effects for other holiday observations during this timeframe which would be more apparent if we had a wider range of samples, but these will not be periodic in nature, as is the case for the previously noted national holiday. 


```{r analysis-availability, fig.align = 'center', fig.width = 8, fig.height = 4, echo = FALSE}

db %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars)) %>%
    ggplot(aes(timestamp, cars_available)) + geom_line(color = "slategray4", size = 1.0) +
    geom_smooth(method="auto", se=TRUE, fullrange=FALSE, level=0.95) +
  ggtitle("Shared Car Availability in Tel Aviv") 

all_cars <- db %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars))

```


Let's dive a little further into the details of this data.

By looking at a single 24-hour period for a weekday, we can observe that the demand for cars starts to occur around 6am, has several local peaks during the day, relaxes around lunch and dinnertime, and the peak demand for the day occurs around 9pm before tapering off and going through the cycle again.

```{r analysis-availability-day, fig.align = 'center', fig.width = 8, fig.height = 4}

sample_day1 <- ymd("2018-12-15")

db_day1 <- db %>% filter(as_date(timestamp) == sample_day1 )


db_day1 %>% group_by(timestamp) %>% summarize(cars_available = sum(total_cars)) %>%
  ggplot(aes(timestamp, cars_available)) + geom_line(color = "slategray4", size = 1.0) +
  ggtitle("Car Availability for Tuesday, December 15, 2018")
  
fleet <-  max(all_cars$cars_available)


```

We'll next take a look in more detail at the apparent periodicity in the sample to confirm our intuition. First, we sort out how the availability varies by the time of day across the entire duration of our data set, to see if the pattern observed above fits the general usage pattern. We see that it is very similar to the pattern exhibited in the single day sample outlined above.

```{r analysis-availability-detail-daily, fig.align = 'center', fig.width = 8, fig.height = 4, echo = FALSE}

# We can plot the utilization of the fleet over time:
dbCarUse <- dbCarUse %>% mutate(utilize = 1 - total_cars/fleet)

dbCarUse %>% group_by(Hour) %>% summarize(cars_available = sum(total_cars)) %>%
    ggplot(aes(Hour, cars_available)) + 
  geom_line(color = "slategray4", size = 2) +
  ggtitle("Shared Car Availability in Tel Aviv by Hour of the Day") 

```

We next analyze the variability by weekday. We do see variability in this plot. Although most of the weekdays are very similar to one another, we note that the demand for cars hits a peak on Thursdays, while the slowest activity is recorded on Saturday, which is a day of religious observance in Israel.

```{r analysis-availability-detail-weekday, fig.align = 'center', fig.width = 8, fig.height = 4, echo = FALSE}

dbCarUse %>% group_by(label_wday) %>% 
    ggplot(aes(label_wday, total_cars, fill = label_wday)) + 
  geom_boxplot(aes(group = label_wday)) +
  ggtitle("Shared Car Availability in Tel Aviv by Weekday") 


```


We also perform a calculation to understand the amount of utilization of the fleet. We see in the data that there is a total of 260 cars. We can use this information to calculate how much of the car fleet is in use at any given time. We note that the maximum utilization is 37%, which means that at least about  two-thirds of the fleet is always idle at any given time. This is likely due to several factors, such as having cars parked in areas where there is little demand at the moment, and the mean time between trip starts from any particular location. In other words, the system will never be highly utilized unless active steps are taken to attempt to match supply with demand at every point. 

```{r analysis-total-fleet}
# Size of our car fleet
fleet

# Minimum number of cars in use
(Utilize_min <- min(dbCarUse$utilize))

# Maximum number of cars in use
(Utilize_max <- max(dbCarUse$utilize))

```


By plotting the overlapped density plots by day, we can get a better understanding of the distribution of this demand. We see that the utilization has two peaks at ~5% utilization ,and ~ 15% utilization. The busiest days also exhibit an additional "shoulder" at about 20%. This type of information can be used to plan maintenance cycles, demand-level pricing models, and so on.


```{r analysis-availability-detail-utilize, fig.align = 'center', fig.width = 8, fig.height = 4, echo = FALSE}

dbCarUse %>% ggplot(aes(x = utilize, fill = label_wday)) + 
  geom_density(position = "identity", alpha = 0.6) +
  ggtitle("Density Plot of Shared Car Fleet Utilization") 

```


Next we analyze the number of cars that visit each of our defined grids over a one-day period to get an idea of the turnover and demand at various locations around Tel Aviv. To do this, we will need to be able to identify the individual cars that visit each location by utilizing the car identifiers in the $carsList vector. We will tally the total number of unique cars found in each grid over the time  period, and then plot that against the geographic coordinates.


```{r analysis-location-day, fig.align = 'center', fig.width = 8, fig.height = 4}

# split out the car identifiers per observation into distinct columns

dbcars <-  db %>% 
  select(carsList) %>% 
  distinct() %>% 
  mutate(cars = sub(carsList, pattern = "\\[", replacement = "")) %>% 
  mutate(cars = sub(cars, pattern = "\\]", replacement = "")) %>% 
  separate(cars, sep = ",", into = c("car1", "car2", "car3", "car4", "car5", "car6", "car7", "car8", "car9", "car10"), fill = "right") %>% 
  mutate_at(2:11, as.numeric)
CarsList <- (1:max(dbcars$car1, na.rm = T))[1:max(dbcars$car1, na.rm = T) %in% dbcars$car1]

# create a list of entries separated by car, and add grid information:

dbByCar <- left_join(db, dbcars) %>% 
  gather(CarI, Car, car1:car10)  %>% 
  filter(!is.na(Car)) %>% 
  group_by(Car) %>% 
  arrange(timestamp) %>%
  ungroup()

# We'll limit our data to a single day to keep it from becoming too unweildy. We will also calculate a summary of the number of cars visited in each grid per hour:


flowByGrid <- dbByCar %>% 
  filter(as_date(timestamp) == sample_day1 ) %>%
  group_by(Grid_lat, Grid_long, Hour) %>%
  summarize(inventory = n_distinct(Car)
  )

flowByGrid %>% ggplot(aes(Grid_long, Grid_lat, color = inventory, size = inventory)) + 
  geom_point() +
  scale_color_gradient2(midpoint = 2.5, low = "blue",  mid = "gold",
                        high = "red", space = "Lab" ) +
  ggtitle("Car Availability by location (grid)")
 
```


And through the magic of ggmaps, we can plot these locations over a map of the city to see how these patterns map onto the neighborhoods of the city.

```{r analysis-location-map, echo = FALSE}

# First, define the box for Tel Aviv:

tav_top <- 32.1500
tav_left <- 34.7350
tav_bot <- 32.0300
tav_right <- 34.8500

tav_borders <- c(left = tav_left,
                 right = tav_right,
                 top = tav_top,
                 bottom = tav_bot)

tav_map <- get_stamenmap(bbox = tav_borders, zoom = 12)

p <- ggmap(tav_map)

p + 
  geom_point(aes(Grid_long, Grid_lat, color = inventory, size = inventory), data = flowByGrid) +
  scale_color_gradient2(midpoint = 2.5, low = "blue",  mid = "gold",
                        high = "red", space = "Lab" ) +
    ggtitle("Shared Car Distribution in Tel Aviv 12/18/2018")

```

#### Availability patterns

Let's look at the distribution of the total number of cars by location. We can see from this chart that once we remove the geographic clustering information, the distribution is not random but follows a power law distribution, with some locations having many more cars fill spots than others. 

```{r analysis-location-dist, fig.align = 'center', fig.width = 8, fig.height = 4}

db_cars <- db_day1 %>% group_by(gridKey) %>% summarize(cars_available = sum(total_cars))

db_cars$gridKey <- factor(db_cars$gridKey, levels = db_cars$gridKey[order(-db_cars$cars_available)])

ggplot(data = db_cars, aes(x = gridKey, y = cars_available)) + 
  geom_bar(stat = "identity", color = "slategray4")

```


### Availability Prediction

Now that we have gotten some idea of the usage patterns in the data, we want to build a prediction system for cars in specific locations.

#### Training and Test Sets

We start by building a series of training and test sets. Since the number of observations in the full sample is fairly large, we will also create two smaller data sets, one which incorporates data from a single day, and another that includes observations for a single week. We will use these sets to compare the models performance against these different time epochs, as well as provide a more convenient vehicle for experimentation of various ML methods. 



```{r prediction-testsets}

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db$total_cars, times = 1, p = 0.1, list = FALSE)
db_train <- db[-test_index,]
db_test <- db[test_index,]

db_test <- db_test %>% 
  semi_join(db_train, by = "gridKey") %>%
  semi_join(db_train, by = "Hour") %>%
  semi_join(db_train, by = "Weekday")

# We will now create a single-day data set, and do the same thing:

sample_day1 <- ymd("2018-12-15")

db_day <- db %>% filter(as_date(timestamp) == sample_day1 )

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db_day$total_cars, times = 1, p = 0.1, list = FALSE)
db_day_train <- db_day[-test_index,]
db_day_test <- db_day[test_index,]

db_day_test <- db_day_test %>% 
  semi_join(db_day_train, by = "gridKey") %>%
  semi_join(db_day_train, by = "Hour") %>%
  semi_join(db_day_train, by = "Weekday")

# and a 1-week data set, from 12/20/18 through 12/26/18:
week_day1 <- ymd("2018-12-20")
week_day7 <- ymd("2018-12-26")

db_week <- db %>% filter(timestamp >= week_day1 & timestamp <= week_day7 )

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = db_week$total_cars, times = 1, p = 0.1, list = FALSE)
db_week_train <- db_week[-test_index,]
db_week_test <- db_week[test_index,]

db_week_test <- db_week_test %>% 
  semi_join(db_week_train, by = "gridKey") %>%
  semi_join(db_week_train, by = "Hour") %>%
  semi_join(db_week_train, by = "Weekday")

```

#### Prediction Model

Now that we have our test sets, we will start building up a prediction model. 

```{r prediction-initial}

# function for calculating RMSE of prediction vs. actual

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}


```


We now will build a predictor, utilizing a linear regression model. Since we have detected variability associated with location, time of day, and day of week, we will select these as factors for our calculation.

##### Single day model

We begin by building up a model utilizing our one-day sample. We calculate a baseline mean, and then add hour of the day and location (by grid) effects to see how these affect the accuracy of the model.


```{r prediction-linear-day}

# build up a model for the day data set, start with a baseline mean:

db_dayTrainMean <- db_day_train %>% summarize(Mean = mean(total_cars)) 
mu_db_dayTrain <- mean(db_day_train$total_cars)

baseline_day_rmse <- RMSE(db_day_test$total_cars, mu_db_dayTrain)


# Now add the hour effect:


day_hour_avgs <- db_day_train %>% 
  group_by(Hour) %>% 
  summarize(b_i_day = mean(total_cars - mu_db_dayTrain))

# day_hour_avgs %>% qplot(b_i, geom ="histogram", data = ., color = I("slategray4"))


predicted_ratings_day <- mu_db_dayTrain + db_day_test %>% 
  left_join(day_hour_avgs, by='Hour') %>%
  pull(b_i_day)

model_1_day_rmse <- RMSE(predicted_ratings_day, db_day_test$total_cars)

# Then we add the location effect:

day_grid_avgs <- db_day_train %>% 
  left_join(day_hour_avgs, by = 'Hour') %>%
  group_by(gridKey) %>% 
  summarize(b_u_day = mean(total_cars - mu_db_dayTrain - b_i_day))

predicted_ratings_day <- db_day_test %>% 
  left_join(day_hour_avgs, by='Hour') %>%
  left_join(day_grid_avgs, by='gridKey') %>%
  mutate(pred = mu_db_dayTrain + b_i_day + b_u_day) %>%
  pull(pred)


model_2_day_rmse <- RMSE(predicted_ratings_day, db_day_test$total_cars)

```

We now put all of our modeling results into a table for comparison

We start with our analysis for a single day. In these results, we see that the hour
of the day factor does not improve the prediction result, but by adding
in information about the location we do significantly improve our result.

```{r results-day}

rmse_day_results <- tibble(method = "One Day Baseline (mean)", RMSE = baseline_day_rmse)

rmse_day_results <- bind_rows(rmse_day_results,
                  tibble(method="One Day Hour Effect Model",  
                  RMSE = model_1_day_rmse)
                  )

rmse_day_results <- bind_rows(rmse_day_results,
                  tibble(method="One Day Location + Hour Effects Model",  
                  RMSE = model_2_day_rmse)
                  )

rmse_day_results %>% knitr::kable()


```


##### Single week model

We will now develop a similar model, expanding our data to a full week of samples. We also add a calculation for the day of week effects to see how significant that might be. 

```{r prediction-linear-week, echo = FALSE}

# Now we do the same modeling for the week data set, and the full data set:

db_weekTrainMean <- db_week_train %>% summarize(Mean = mean(total_cars)) 
mu_db_weekTrain <- mean(db_week_train$total_cars)

baseline_week_rmse <- RMSE(db_week_test$total_cars, mu_db_weekTrain)


# First, the hour effect:


week_hour_avgs <- db_week_train %>% 
  group_by(Hour) %>% 
  summarize(b_i_week = mean(total_cars - mu_db_weekTrain))

# week_hour_avgs %>% qplot(b_i_week, geom ="histogram", data = ., color = I("slategray4"))


predicted_ratings_week <- mu_db_weekTrain + db_week_test %>% 
  left_join(week_hour_avgs, by='Hour') %>%
  pull(b_i_week)

model_1_week_rmse <- RMSE(predicted_ratings_week, db_week_test$total_cars)

# Then we add the location effect:

week_grid_avgs <- db_week_train %>% 
  left_join(week_hour_avgs, by = 'Hour') %>%
  group_by(gridKey) %>% 
  summarize(b_u_week = mean(total_cars - mu_db_weekTrain - b_i_week))

predicted_ratings_week <- db_week_test %>% 
  left_join(week_hour_avgs, by='Hour') %>%
  left_join(week_grid_avgs, by='gridKey') %>%
  mutate(pred = mu_db_dayTrain + b_i_week + b_u_week) %>%
  pull(pred)


model_2_week_rmse <- RMSE(predicted_ratings_week, db_week_test$total_cars)

# Then the weekday effect:

week_day_avgs <- db_week_train %>% 
  left_join(week_hour_avgs, by = 'Hour') %>%
  left_join(week_grid_avgs, by='gridKey') %>%
  group_by(Weekday) %>% 
  summarize(b_d_week = mean(total_cars - mu_db_weekTrain - b_i_week - b_u_week))

predicted_ratings_week <- db_week_test %>% 
  left_join(week_hour_avgs, by='Hour') %>%
  left_join(week_grid_avgs, by='gridKey') %>%
  left_join(week_day_avgs, by = 'Weekday') %>%
  mutate(pred = mu_db_dayTrain + b_i_week + b_u_week + b_d_week) %>%
  pull(pred)

model_3_week_rmse <- RMSE(predicted_ratings_week, db_week_test$total_cars)

```


We now look at the results for a full week. Similar to our first set of results, we see that the
time of day factor is not very important, while the location remains significant. We do see a
slight degradation as compared to the single day, this might be attributable to different usage 
patterns between weekdays and weekends, for example. 

```{r results-week}


rmse_week_results <- tibble(method = "One Week Baseline (mean)", RMSE = baseline_week_rmse)

                             
rmse_week_results <- bind_rows(rmse_week_results,
                  tibble(method="One Week Hour Effect Model",  
                  RMSE = model_1_week_rmse)
                  )                                    

rmse_week_results <- bind_rows(rmse_week_results,
                  tibble(method="One Week Location + Hour Effects Model",  
                  RMSE = model_2_week_rmse)
                  )                                    

rmse_week_results <- bind_rows(rmse_week_results,
                  tibble(method="One Week Weekday + Location + Hour Effects Model",  
                  RMSE = model_3_week_rmse)
                  )                                    

rmse_week_results %>% knitr::kable()

```


##### Full sample model

We now proceed to expand our model to include the full set of data. This data will include the previously noted holidays as well as several weeks of daily and weekly patterns. 


```{r prediction-linear-full, echo = FALSE}


dbTrainMean <- db_train %>% summarize(Mean = mean(total_cars)) 
mu_dbTrain <- mean(db_train$total_cars)

baseline_rmse <- RMSE(db_test$total_cars, mu_dbTrain)

# First, the hour effect:


hour_avgs <- db_train %>% 
  group_by(Hour) %>% 
  summarize(b_i = mean(total_cars - mu_dbTrain))

# hour_avgs %>% qplot(b_i, geom ="histogram", data = ., color = I("slategray4"))


predicted_ratings <- mu_dbTrain + db_test %>% 
  left_join(hour_avgs, by='Hour') %>%
  pull(b_i)

model_1_rmse <- RMSE(predicted_ratings, db_test$total_cars)

# Then we add the location effect:

grid_avgs <- db_train %>% 
  left_join(hour_avgs, by = 'Hour') %>%
  group_by(gridKey) %>% 
  summarize(b_u = mean(total_cars - mu_dbTrain - b_i))

predicted_ratings <- db_test %>% 
  left_join(hour_avgs, by='Hour') %>%
  left_join(grid_avgs, by='gridKey') %>%
  mutate(pred = mu_dbTrain + b_i + b_u) %>%
  pull(pred)


model_2_rmse <- RMSE(predicted_ratings, db_test$total_cars)

# Then the weekday effect:

day_avgs <- db_train %>% 
  left_join(hour_avgs, by = 'Hour') %>%
  left_join(grid_avgs, by='gridKey') %>%
  group_by(Weekday) %>% 
  summarize(b_d = mean(total_cars - mu_dbTrain - b_i - b_u))

predicted_ratings <- db_test %>% 
  left_join(hour_avgs, by='Hour') %>%
  left_join(grid_avgs, by='gridKey') %>%
  left_join(day_avgs, by = 'Weekday') %>%
  mutate(pred = mu_dbTrain + b_i + b_u + b_d) %>%
  pull(pred)

model_3_rmse <- RMSE(predicted_ratings, db_test$total_cars)


```

In our model with the full set of data, we see the same basic pattern as in the previous two
models, with further degradation in the accuracy as more data with presumably additional variation 
of usage patterns. We noted at the outset that this time period, coming as it does over the end of the year, includes various religious and national holidays, and presumably travel patterns that are not regular. 

```{r results-full}



rmse_results <- tibble(method = "Full Sample Baseline (mean)", RMSE = baseline_rmse)


rmse_results <- bind_rows(rmse_results,
                  tibble(method="Full Sample Hour Effect Model",  
                  RMSE = model_1_rmse)
                  )                                    

rmse_results <- bind_rows(rmse_results,
                  tibble(method="Full Sample Location + Hour Effects Model",  
                  RMSE = model_2_rmse)
                  )       

rmse_results <- bind_rows(rmse_results,
                  tibble(method="Full Sample Weekday + Location + Hour Effects Model",  
                  RMSE = model_3_rmse)
                  )       

rmse_results %>% knitr::kable()


```

##### Prediction Model Using knn

We see from the above that throughout the different datasets we are able to improve our modeling accuracy by including the time of day and location factors. The differences from day to day do not seem to help. 

We will now experiment with a knn model. Since the demand for cars is likely to be very similar in the locations immediately adjacent to any given spot, and vary slowly as the spots are traversed, it seems intuitive that this technique may help with accuracy by including some neighbor information in our model. 

However, it turns out that this model is computationally intensive with large numbers of data points, so for the purposes of this research, we will limit our experiment to the one day sample that we have been using in some of our previous models. 

```{r prediction-knn}

# Now develop our knn model and prediction
# Model: day

train_knnD <- train(total_cars ~ Hour + gridKey, method = "knn", data = db_day_train)

train_knnD

plot(train_knnD)

train_knnD$bestTune

y_hat_knnD <- predict(train_knnD, db_day_test, type = "raw")

model_knn_day <- RMSE(y_hat_knnD, db_day_test$total_cars)


```

We see that the best performance of the model is with a small number of neighbors. This seems intuitive, as the demand for cars would likely be very similar with adjacent locations, but the similarity would fall off the further away the nighbor becomes. This neighbor information does improve our RMSE significantly, as we see in the table below.


```{r results-knn}


rmse_knn_results <- tibble(method="One Day knn Model",  
                                     RMSE = model_knn_day)
       

rmse_knn_results %>% knitr::kable()


```

### Plotting the predicted results

Now that we have our predictions, let's compare a plot of our results and compare that to our earlier plot of availability. 

With our first model, we can see the patterns of usage similar to the actual data beginning to emerge. However the range of predicted available cars is more restricted than in the actual data. 

```{r model-map, echo = FALSE}

m <- db_day_test %>% 
  left_join(day_hour_avgs, by='Hour') %>%
  left_join(day_grid_avgs, by='gridKey') %>%
  mutate(pred = mu_db_dayTrain + b_i_day + b_u_day) %>%
  select(Grid_long, Grid_lat, pred)


m %>% ggplot(aes(Grid_long, Grid_lat, color = pred, size = pred)) + 
  geom_point() +
  scale_color_gradient2(midpoint = 2.5, low = "blue",  mid = "gold",
                        high = "red", space = "Lab" ) +
    ggtitle("Car Availability Model ") 

```

When we look at the knn model data, however, we see that the same clustering patterns that we saw in our data analysis are now more visible in the plot, and the relative scale of availability more closely matches the actual data. Overall the knn model provides the best RMSE and most accurate representation of the pattern of usage that we are attempting to predict. More work remains in data reduction techniques in order to be able to generalize this work for larger samples of data.


```{r model-map-knn, echo = FALSE}

q <- m %>% select(-pred) %>% mutate(kpred = y_hat_knnD)

q %>% ggplot(aes(Grid_long, Grid_lat, color = kpred, size = kpred)) + 
  geom_point() +
  scale_color_gradient2(midpoint = 2.5, low = "blue",  mid = "gold",
                        high = "red", space = "Lab" ) +
  ggtitle("Car Availability Model Using knn") 



```






